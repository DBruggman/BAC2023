{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Metrics and auxiliar libraries from sklearn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Some auxiliary functions for scoring and tuning\n",
    "import scoring_utils, tuning_utils\n",
    "\n",
    "#DEV\n",
    "import importlib as imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataframe cleaned during the feature importance process.\n",
    "df = pd.read_csv('../data/data_clean.csv')\n",
    "target = 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Add UrbanRural one-hot encoded version diretly to the dataframe. That encoding is straighforward.\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_urban_rural = encoder.fit_transform(df[['UrbanRural']])\n",
    "encoded_urban_rural = pd.DataFrame(encoded_urban_rural, columns=encoder.get_feature_names_out(['UrbanRural']))\n",
    "\n",
    "encoded_urban_rural.index = df.index\n",
    "\n",
    "# Concatenating the encoded DataFrame with the original DataFrame\n",
    "df = pd.concat([df, encoded_urban_rural], axis=1)\n",
    "\n",
    "all_features = [feature for feature in df.columns if feature not in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add count encoded features\n",
    "experimental_features = ['City', 'State', 'Bank', 'ApprovalFY', 'NAICS_i', 'FranchiseCode']\n",
    "features = [f for f in all_features if f not in experimental_features]\n",
    "# Count encoding\n",
    "count_encoded_features = ['City', 'Bank', 'State']\n",
    "features_count_encoding = features + count_encoded_features\n",
    "\n",
    "for feature in count_encoded_features:\n",
    "    df[feature + 'Loans'] = df.groupby(feature)[feature].transform('count')\n",
    "    df[feature + 'Loans'].fillna(0, inplace=True)\n",
    "\n",
    "    features_count_encoding.remove(feature)\n",
    "    features_count_encoding.append(feature+'Loans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for and remove outliers\n",
    "\n",
    "def remove_outliers(df:pd.DataFrame, c:str):\n",
    "    z_scores = np.abs((df[c] - df[c].mean())/df[c].std())\n",
    "    no_outliers = df[z_scores < 3]\n",
    "    return no_outliers\n",
    "\n",
    "#remove outliers from features\n",
    "\n",
    "have_outliers = ['Term','NoEmp','CreateJob','RetainedJob','GrAppv','SBA_Appv']\n",
    "\n",
    "for col in have_outliers:\n",
    "    # print(f\"\\n\\nCol: {col}\")\n",
    "    # print(\"\\nBefore:\")\n",
    "    # for s in df[col].describe().to_string().split('\\n'):\n",
    "    #     print(f'\\t{s}')\n",
    "    df_filtered = remove_outliers(df,col)\n",
    "    # print(\"\\nAfter:\")\n",
    "    # for s in df[col].describe().to_string().split('\\n'):\n",
    "    #     print(f'\\t{s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic transform to selected columns\n",
    "log_columns = ['Term', 'DisbursementGross', 'GrAppv', 'SBA_Appv','CreateJob']\n",
    "for col in log_columns:\n",
    "    df[col] = np.log1p(df[col])  # Using np.log1p to avoid log(0) issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = df_filtered[features_count_encoding]\n",
    "y_total = df_filtered[target]\n",
    "\n",
    "X_train, X_, y_train, y_ = train_test_split(X_total, y_total, train_size=.8)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_, y_, train_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cv_scaled = scaler.transform(X_cv)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_total_scaled = pd.DataFrame(scaler.transform(X_total), columns=features_count_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>RetainedJob</th>\n",
       "      <th>UrbanRural</th>\n",
       "      <th>RevLineCr</th>\n",
       "      <th>LowDoc</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>isNewBusiness</th>\n",
       "      <th>isFranchise</th>\n",
       "      <th>SBARatio</th>\n",
       "      <th>InterestRate</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>CityLoans</th>\n",
       "      <th>BankLoans</th>\n",
       "      <th>StateLoans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111147</th>\n",
       "      <td>0.137865</td>\n",
       "      <td>-0.054494</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.015807</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>1.151649</td>\n",
       "      <td>1.362151</td>\n",
       "      <td>1.313584</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.241124</td>\n",
       "      <td>0.474840</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.505395</td>\n",
       "      <td>-0.384514</td>\n",
       "      <td>-0.521525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550731</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>0.053630</td>\n",
       "      <td>-0.022887</td>\n",
       "      <td>-0.032331</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>1.811060</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>0.320145</td>\n",
       "      <td>0.364542</td>\n",
       "      <td>-0.031260</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>-1.007206</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>3.257084</td>\n",
       "      <td>-0.636427</td>\n",
       "      <td>0.387531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656850</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.040593</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>1.811060</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.632714</td>\n",
       "      <td>-0.665101</td>\n",
       "      <td>-0.700706</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>-1.345698</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.209683</td>\n",
       "      <td>2.219625</td>\n",
       "      <td>-0.488894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166584</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.122072</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.036462</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>1.811060</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.767553</td>\n",
       "      <td>-0.742896</td>\n",
       "      <td>-0.751287</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>0.593770</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>1.160668</td>\n",
       "      <td>2.219625</td>\n",
       "      <td>0.703957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140237</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.095041</td>\n",
       "      <td>-0.027024</td>\n",
       "      <td>-0.028200</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.758564</td>\n",
       "      <td>-0.733743</td>\n",
       "      <td>-0.724509</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.820509</td>\n",
       "      <td>0.593770</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.231624</td>\n",
       "      <td>0.077539</td>\n",
       "      <td>-0.899749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238238</th>\n",
       "      <td>2.453678</td>\n",
       "      <td>-0.149103</td>\n",
       "      <td>-0.018751</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>0.765112</td>\n",
       "      <td>0.817584</td>\n",
       "      <td>0.263297</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>0.369633</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>2.401363</td>\n",
       "      <td>1.006203</td>\n",
       "      <td>-0.301260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20123</th>\n",
       "      <td>2.453678</td>\n",
       "      <td>-0.095041</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>-1.153145</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>2.627415</td>\n",
       "      <td>-0.455177</td>\n",
       "      <td>-0.424851</td>\n",
       "      <td>-0.406150</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.530816</td>\n",
       "      <td>0.712699</td>\n",
       "      <td>1.310253</td>\n",
       "      <td>-1.031660</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.400176</td>\n",
       "      <td>-0.774399</td>\n",
       "      <td>-0.521525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389210</th>\n",
       "      <td>-0.479686</td>\n",
       "      <td>-0.068010</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>2.627415</td>\n",
       "      <td>-0.623725</td>\n",
       "      <td>-0.596458</td>\n",
       "      <td>-0.584669</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.530816</td>\n",
       "      <td>1.174695</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.225142</td>\n",
       "      <td>-0.778316</td>\n",
       "      <td>0.703957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659150</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.040593</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>1.811060</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.758564</td>\n",
       "      <td>-0.733743</td>\n",
       "      <td>-0.745336</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>-1.345698</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.352303</td>\n",
       "      <td>1.006203</td>\n",
       "      <td>0.703957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789174</th>\n",
       "      <td>1.681740</td>\n",
       "      <td>-0.095041</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.028200</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.138306</td>\n",
       "      <td>-0.102229</td>\n",
       "      <td>0.105605</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>1.689585</td>\n",
       "      <td>-0.417132</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.435581</td>\n",
       "      <td>-0.739143</td>\n",
       "      <td>2.193773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802042</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>-1.153145</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.533833</td>\n",
       "      <td>-0.504934</td>\n",
       "      <td>-0.489459</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.530816</td>\n",
       "      <td>0.621215</td>\n",
       "      <td>1.310253</td>\n",
       "      <td>-1.031660</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.210680</td>\n",
       "      <td>-0.749765</td>\n",
       "      <td>-0.521525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603195</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>-0.031160</td>\n",
       "      <td>-0.040593</td>\n",
       "      <td>-1.153145</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.781037</td>\n",
       "      <td>-0.756624</td>\n",
       "      <td>-0.749799</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.820509</td>\n",
       "      <td>-1.231343</td>\n",
       "      <td>1.310253</td>\n",
       "      <td>-1.031660</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>0.131409</td>\n",
       "      <td>-0.667163</td>\n",
       "      <td>2.193773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>1.811060</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.713618</td>\n",
       "      <td>-0.687982</td>\n",
       "      <td>-0.715583</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>-1.153581</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.348812</td>\n",
       "      <td>2.219625</td>\n",
       "      <td>2.193773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632085</th>\n",
       "      <td>-0.492551</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>-0.040593</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>0.724188</td>\n",
       "      <td>0.867922</td>\n",
       "      <td>0.831583</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.241124</td>\n",
       "      <td>-1.345698</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.319889</td>\n",
       "      <td>-0.762873</td>\n",
       "      <td>-0.428752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829136</th>\n",
       "      <td>-0.827058</td>\n",
       "      <td>-0.135587</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.040593</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.781037</td>\n",
       "      <td>-0.756624</td>\n",
       "      <td>-0.749799</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>0.820509</td>\n",
       "      <td>0.095180</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>1.097835</td>\n",
       "      <td>0.077539</td>\n",
       "      <td>2.193773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65930</th>\n",
       "      <td>-1.161564</td>\n",
       "      <td>0.080661</td>\n",
       "      <td>-0.031160</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.578779</td>\n",
       "      <td>-0.550696</td>\n",
       "      <td>-0.626323</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>0.021993</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>0.176788</td>\n",
       "      <td>0.050946</td>\n",
       "      <td>-0.601663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31242</th>\n",
       "      <td>-0.878520</td>\n",
       "      <td>-0.122072</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.036462</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.466414</td>\n",
       "      <td>-0.436291</td>\n",
       "      <td>-0.551940</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>-0.604675</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.516865</td>\n",
       "      <td>0.436725</td>\n",
       "      <td>-0.194502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640259</th>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.054494</td>\n",
       "      <td>-0.027024</td>\n",
       "      <td>-0.015807</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.691144</td>\n",
       "      <td>-0.665101</td>\n",
       "      <td>-0.700706</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>-1.350272</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.386711</td>\n",
       "      <td>0.050946</td>\n",
       "      <td>0.387531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761590</th>\n",
       "      <td>2.144903</td>\n",
       "      <td>-0.068010</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>-1.153145</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.069539</td>\n",
       "      <td>-0.032214</td>\n",
       "      <td>-0.094872</td>\n",
       "      <td>-0.629338</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-0.048568</td>\n",
       "      <td>0.817906</td>\n",
       "      <td>1.310253</td>\n",
       "      <td>-1.031660</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>1.589525</td>\n",
       "      <td>-0.535858</td>\n",
       "      <td>-0.838435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420926</th>\n",
       "      <td>-1.251623</td>\n",
       "      <td>-0.108556</td>\n",
       "      <td>-0.031160</td>\n",
       "      <td>-0.032331</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>-0.552163</td>\n",
       "      <td>-0.380602</td>\n",
       "      <td>-0.758564</td>\n",
       "      <td>-0.733743</td>\n",
       "      <td>-0.745336</td>\n",
       "      <td>1.588970</td>\n",
       "      <td>-0.244781</td>\n",
       "      <td>-1.207336</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>-0.763211</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>-0.362837</td>\n",
       "      <td>-0.481958</td>\n",
       "      <td>-0.765208</td>\n",
       "      <td>-0.899749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Term     NoEmp  CreateJob  RetainedJob  UrbanRural  RevLineCr  \\\n",
       "111147  0.137865 -0.054494  -0.035296    -0.015807    0.387977  -0.552163   \n",
       "550731 -0.325298  0.053630  -0.022887    -0.032331    0.387977   1.811060   \n",
       "656850 -0.325298 -0.135587  -0.035296    -0.040593    0.387977   1.811060   \n",
       "166584 -0.325298 -0.122072  -0.035296    -0.036462    0.387977   1.811060   \n",
       "140237 -0.325298 -0.095041  -0.027024    -0.028200    0.387977  -0.552163   \n",
       "238238  2.453678 -0.149103  -0.018751    -0.044724    0.387977  -0.552163   \n",
       "20123   2.453678 -0.095041  -0.035296    -0.044724   -1.153145  -0.552163   \n",
       "389210 -0.479686 -0.068010  -0.035296    -0.044724    0.387977  -0.552163   \n",
       "659150 -0.325298 -0.135587  -0.035296    -0.040593    0.387977   1.811060   \n",
       "789174  1.681740 -0.095041  -0.035296    -0.028200    0.387977  -0.552163   \n",
       "802042 -0.325298 -0.135587  -0.035296    -0.044724   -1.153145  -0.552163   \n",
       "603195 -0.325298 -0.135587  -0.031160    -0.040593   -1.153145  -0.552163   \n",
       "5528   -0.325298 -0.135587  -0.035296    -0.044724    0.387977   1.811060   \n",
       "632085 -0.492551 -0.135587   0.001930    -0.040593    0.387977  -0.552163   \n",
       "829136 -0.827058 -0.135587  -0.035296    -0.040593    0.387977  -0.552163   \n",
       "65930  -1.161564  0.080661  -0.031160     0.025504    0.387977  -0.552163   \n",
       "31242  -0.878520 -0.122072  -0.035296    -0.036462    0.387977  -0.552163   \n",
       "640259 -0.325298 -0.054494  -0.027024    -0.015807    0.387977  -0.552163   \n",
       "761590  2.144903 -0.068010  -0.035296    -0.044724   -1.153145  -0.552163   \n",
       "420926 -1.251623 -0.108556  -0.031160    -0.032331    0.387977  -0.552163   \n",
       "\n",
       "          LowDoc  DisbursementGross    GrAppv  SBA_Appv  isNewBusiness  \\\n",
       "111147 -0.380602           1.151649  1.362151  1.313584       1.588970   \n",
       "550731 -0.380602           0.320145  0.364542 -0.031260      -0.629338   \n",
       "656850 -0.380602          -0.632714 -0.665101 -0.700706      -0.629338   \n",
       "166584 -0.380602          -0.767553 -0.742896 -0.751287       1.588970   \n",
       "140237 -0.380602          -0.758564 -0.733743 -0.724509       1.588970   \n",
       "238238 -0.380602           0.765112  0.817584  0.263297       1.588970   \n",
       "20123   2.627415          -0.455177 -0.424851 -0.406150      -0.629338   \n",
       "389210  2.627415          -0.623725 -0.596458 -0.584669      -0.629338   \n",
       "659150 -0.380602          -0.758564 -0.733743 -0.745336      -0.629338   \n",
       "789174 -0.380602          -0.138306 -0.102229  0.105605      -0.629338   \n",
       "802042 -0.380602          -0.533833 -0.504934 -0.489459       1.588970   \n",
       "603195 -0.380602          -0.781037 -0.756624 -0.749799       1.588970   \n",
       "5528   -0.380602          -0.713618 -0.687982 -0.715583      -0.629338   \n",
       "632085 -0.380602           0.724188  0.867922  0.831583      -0.629338   \n",
       "829136 -0.380602          -0.781037 -0.756624 -0.749799       1.588970   \n",
       "65930  -0.380602          -0.578779 -0.550696 -0.626323      -0.629338   \n",
       "31242  -0.380602          -0.466414 -0.436291 -0.551940      -0.629338   \n",
       "640259 -0.380602          -0.691144 -0.665101 -0.700706      -0.629338   \n",
       "761590 -0.380602          -0.069539 -0.032214 -0.094872      -0.629338   \n",
       "420926 -0.380602          -0.758564 -0.733743 -0.745336       1.588970   \n",
       "\n",
       "        isFranchise  SBARatio  InterestRate  UrbanRural_0  UrbanRural_1  \\\n",
       "111147    -0.244781  0.241124      0.474840     -0.763211      0.969311   \n",
       "550731    -0.244781 -1.207336     -1.007206     -0.763211      0.969311   \n",
       "656850    -0.244781 -1.207336     -1.345698     -0.763211      0.969311   \n",
       "166584    -0.244781 -1.207336      0.593770     -0.763211      0.969311   \n",
       "140237    -0.244781  0.820509      0.593770     -0.763211      0.969311   \n",
       "238238    -0.244781 -1.207336      0.369633     -0.763211      0.969311   \n",
       "20123     -0.244781  0.530816      0.712699      1.310253     -1.031660   \n",
       "389210    -0.244781  0.530816      1.174695     -0.763211      0.969311   \n",
       "659150    -0.244781 -1.207336     -1.345698     -0.763211      0.969311   \n",
       "789174    -0.244781  1.689585     -0.417132     -0.763211      0.969311   \n",
       "802042    -0.244781  0.530816      0.621215      1.310253     -1.031660   \n",
       "603195    -0.244781  0.820509     -1.231343      1.310253     -1.031660   \n",
       "5528      -0.244781 -1.207336     -1.153581     -0.763211      0.969311   \n",
       "632085    -0.244781  0.241124     -1.345698     -0.763211      0.969311   \n",
       "829136    -0.244781  0.820509      0.095180     -0.763211      0.969311   \n",
       "65930     -0.244781 -1.207336      0.021993     -0.763211      0.969311   \n",
       "31242     -0.244781 -1.207336     -0.604675     -0.763211      0.969311   \n",
       "640259    -0.244781 -1.207336     -1.350272     -0.763211      0.969311   \n",
       "761590    -0.244781 -0.048568      0.817906      1.310253     -1.031660   \n",
       "420926    -0.244781 -1.207336      0.703551     -0.763211      0.969311   \n",
       "\n",
       "        UrbanRural_2  CityLoans  BankLoans  StateLoans  \n",
       "111147     -0.362837  -0.505395  -0.384514   -0.521525  \n",
       "550731     -0.362837   3.257084  -0.636427    0.387531  \n",
       "656850     -0.362837  -0.209683   2.219625   -0.488894  \n",
       "166584     -0.362837   1.160668   2.219625    0.703957  \n",
       "140237     -0.362837  -0.231624   0.077539   -0.899749  \n",
       "238238     -0.362837   2.401363   1.006203   -0.301260  \n",
       "20123      -0.362837  -0.400176  -0.774399   -0.521525  \n",
       "389210     -0.362837  -0.225142  -0.778316    0.703957  \n",
       "659150     -0.362837  -0.352303   1.006203    0.703957  \n",
       "789174     -0.362837  -0.435581  -0.739143    2.193773  \n",
       "802042     -0.362837  -0.210680  -0.749765   -0.521525  \n",
       "603195     -0.362837   0.131409  -0.667163    2.193773  \n",
       "5528       -0.362837  -0.348812   2.219625    2.193773  \n",
       "632085     -0.362837  -0.319889  -0.762873   -0.428752  \n",
       "829136     -0.362837   1.097835   0.077539    2.193773  \n",
       "65930      -0.362837   0.176788   0.050946   -0.601663  \n",
       "31242      -0.362837  -0.516865   0.436725   -0.194502  \n",
       "640259     -0.362837  -0.386711   0.050946    0.387531  \n",
       "761590     -0.362837   1.589525  -0.535858   -0.838435  \n",
       "420926     -0.362837  -0.481958  -0.765208   -0.899749  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total_scaled.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features have been succesfully scaled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(activation='relu', units=16),\n",
    "    tf.keras.layers.Dense(activation='relu', units=32),\n",
    "    tf.keras.layers.Dense(activation='sigmoid', units=64),\n",
    "    tf.keras.layers.Dense(activation='relu', units=8),\n",
    "    tf.keras.layers.Dense(activation='sigmoid', units=1)\n",
    "])\n",
    "\n",
    "network.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21321/21321 [==============================] - 14s 638us/step - loss: 0.4477 - val_loss: 0.4844\n",
      "Epoch 2/10\n",
      "21321/21321 [==============================] - 13s 589us/step - loss: 0.4627 - val_loss: 0.4796\n",
      "Epoch 3/10\n",
      "21321/21321 [==============================] - 14s 654us/step - loss: 0.4625 - val_loss: 0.4786\n",
      "Epoch 4/10\n",
      "21321/21321 [==============================] - 13s 588us/step - loss: 0.4625 - val_loss: 0.4725\n",
      "Epoch 5/10\n",
      "21321/21321 [==============================] - 12s 586us/step - loss: 0.4624 - val_loss: 0.4656\n",
      "Epoch 6/10\n",
      "21321/21321 [==============================] - 13s 590us/step - loss: 0.4624 - val_loss: 0.4721\n",
      "Epoch 7/10\n",
      "21321/21321 [==============================] - 13s 587us/step - loss: 0.4624 - val_loss: 0.4589\n",
      "Epoch 8/10\n",
      "21321/21321 [==============================] - 14s 655us/step - loss: 0.4624 - val_loss: 0.4702\n",
      "Epoch 9/10\n",
      "21321/21321 [==============================] - 13s 589us/step - loss: 0.4624 - val_loss: 0.4714\n",
      "Epoch 10/10\n",
      "21321/21321 [==============================] - 13s 589us/step - loss: 0.4624 - val_loss: 0.4648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2febfb210>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_cv_scaled, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Assuming 'input_shape' is the number of features\n",
    "\n",
    "# Wide part of the network (linear relationships)\n",
    "input_layer = Input(shape=(20,))\n",
    "wide_branch = Dense(128, activation='relu')(input_layer)\n",
    "wide_branch = Dense(64, activation='relu')(wide_branch)\n",
    "\n",
    "# Deep part of the network (non-linear relationships)\n",
    "deep_branch = Dense(64, activation='relu')(input_layer)\n",
    "deep_branch = Dense(128, activation='relu')(deep_branch)\n",
    "deep_branch = Dense(64, activation='relu')(deep_branch)\n",
    "\n",
    "# Concatenate wide and deep parts\n",
    "concatenated = Concatenate()([wide_branch, deep_branch])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_cv_scaled, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv_df = pd.DataFrame(y_cv)\n",
    "yhat_cv_df = pd.DataFrame(model.predict(X_cv_scaled), index=y_cv_df.index)\n",
    "\n",
    "pd.concat([\n",
    "    y_cv_df,\n",
    "    yhat_cv_df\n",
    "], axis=1).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_utils.get_metrics(y_cv, yhat_cv_df>.5, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_cv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_utils.get_confusion_matrix(y_cv, yhat_cv_df[0]>.5, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
